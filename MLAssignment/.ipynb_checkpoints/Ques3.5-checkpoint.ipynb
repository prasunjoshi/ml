{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score , precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read csv file into DataFrame\n",
    "Parameters: Path of csv file\n",
    "Returns: Pandas DataFrame \n",
    "\"\"\"  \n",
    "def load_data(csv_path):\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2254 entries, 0 to 2253\n",
      "Data columns (total 8 columns):\n",
      "Unnamed: 0    2254 non-null int64\n",
      "1             2254 non-null float64\n",
      "2             2254 non-null float64\n",
      "3             2254 non-null float64\n",
      "4             2254 non-null float64\n",
      "5             2254 non-null float64\n",
      "6             2254 non-null float64\n",
      "7             2254 non-null int64\n",
      "dtypes: float64(6), int64(2)\n",
      "memory usage: 141.0 KB\n"
     ]
    }
   ],
   "source": [
    "#load D9.csv into dataframe object\n",
    "q3 = load_data(\"dataset/D9.csv\")\n",
    "\n",
    "#have a look at the info of the data\n",
    "q3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the unnamed column as 'id'\n",
    "q3.rename(columns={'Unnamed: 0':'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no. of features\n",
    "n = 6 \n",
    "m = q3.shape[0]\n",
    "\n",
    "X = np.ones((m,n + 1))\n",
    "y = np.array((m,1))\n",
    "\n",
    "X[:,1] = q3['1'].values\n",
    "X[:,2] = q3['2'].values\n",
    "X[:,3] = q3['3'].values\n",
    "X[:,4] = q3['4'].values\n",
    "X[:,5] = q3['5'].values\n",
    "X[:,6] = q3['6'].values\n",
    "\n",
    "#Store Labels\n",
    "y = q3['7'].values\n",
    "\n",
    "#Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 40)\n",
    "\n",
    "#Store species labels\n",
    "class_labels = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression using Library\n",
    "classifier = LogisticRegression(random_state=40)\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  80.91397849462365 %\n"
     ]
    }
   ],
   "source": [
    "#Calculate accuracy of the model\n",
    "accuracy = accuracy_score(y_test,y_pred,normalize=True, sample_weight=None) * 100\n",
    "print(\"Accuracy: \", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8082191780821918\n"
     ]
    }
   ],
   "source": [
    "#Calculate Precision of the model\n",
    "precision = precision_score(y_test,y_pred,sample_weight=None)\n",
    "print(\"Precision: \", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.9966216216216216\n"
     ]
    }
   ],
   "source": [
    "#Calculate recall of the model\n",
    "recall = recall_score(y_test,y_pred,sample_weight=None)\n",
    "print(\"Recall: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.8925869894099848\n"
     ]
    }
   ],
   "source": [
    "#Calculate F1 score of the model\n",
    "f1 = f1_score(y_test,y_pred,sample_weight=None)\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.7712926742532007\n"
     ]
    }
   ],
   "source": [
    "#Calculate AUC\n",
    "y_score = classifier.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_score)\n",
    "print(\"AUC: \", auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
